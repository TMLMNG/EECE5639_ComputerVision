{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "p2",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1zRN4NA7SqTMaG-64AEuHtB9tmhDTswtJ",
      "authorship_tag": "ABX9TyPFPJBwv4c16BWYBYnbkFAC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scheung97/EECE5639_ComputerVision/blob/main/p2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBN8SNrx7x0y"
      },
      "source": [
        " # Northeastern University\n",
        "# EECE5639 Computer Vision\n",
        "\n",
        "# Spenser Cheung & Tim Liming\n",
        "\n",
        "# Project 2\n",
        "\n",
        "import cv2 \n",
        "import numpy as np \n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "from scipy import ndimage\n",
        "from scipy import signal\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import drive\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiyTMG2BRlfi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "outputId": "a24c033a-426b-4510-dac4-5dddc80adc67"
      },
      "source": [
        "!python3 --version\n",
        "!pip install opencv-python==3.4.2.17\n",
        "!pip install opencv-contrib-python==3.4.2.17"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12\n",
            "Collecting opencv-python==3.4.2.17\n",
            "  Downloading opencv_python-3.4.2.17-cp37-cp37m-manylinux1_x86_64.whl (25.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.0 MB 105 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python==3.4.2.17) (1.19.5)\n",
            "Installing collected packages: opencv-python\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed opencv-python-3.4.2.17\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-contrib-python==3.4.2.17\n",
            "  Downloading opencv_contrib_python-3.4.2.17-cp37-cp37m-manylinux1_x86_64.whl (30.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 30.6 MB 26 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python==3.4.2.17) (1.19.5)\n",
            "Installing collected packages: opencv-contrib-python\n",
            "  Attempting uninstall: opencv-contrib-python\n",
            "    Found existing installation: opencv-contrib-python 4.1.2.30\n",
            "    Uninstalling opencv-contrib-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-contrib-python-4.1.2.30\n",
            "Successfully installed opencv-contrib-python-3.4.2.17\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAj3WRw2KJn9",
        "outputId": "4a51e771-fade-4747-c3c3-8833807ff702"
      },
      "source": [
        "# Mount Google Drive in Google Colab\n",
        "\n",
        "# Public 'DanaOffice' folder link: \n",
        "# https://drive.google.com/drive/folders/1WUWYblxSid4YkRJawACK7tNizW79Zk6h?usp=sharing\n",
        "\n",
        "# Public 'DanaHallWay1' folder link: \n",
        "# https://drive.google.com/drive/folders/1Y-nP4B0Usws3yPL4f0T-pGWNcd-o1qGs?usp=sharing\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "office_path = \"/content/gdrive/MyDrive/EECE5639_P2_DanaOffice\"\n",
        "hallway_path = \"/content/gdrive/MyDrive/EECE5639_P2_DanaHallWay1\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e71zLXjZ9l4w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d22f56ee-873e-4b84-c1fe-64e569d34fb1"
      },
      "source": [
        "# i. Read in two images.\n",
        "\n",
        "# Office\n",
        "office_files = [filename for filename in sorted(os.listdir(office_path)) if os.path.isfile(os.path.join(office_path, filename))]\n",
        "\n",
        "office_images = []    \n",
        "for i in range(0, len(office_files)): \n",
        "    img = cv2.imread(os.path.join(office_path, office_files[i]))\n",
        "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGRA2GRAY)\n",
        "    office_images.append(gray_img)\n",
        "\n",
        "office_images = np.array(office_images)\n",
        "print(office_images.shape)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 340, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzWMkJ1GUNza",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc404eda-6050-463b-cf80-5aca7cb99916"
      },
      "source": [
        "# i. Read in two images.\n",
        "\n",
        "# Hallway\n",
        "hallway_files = [filename for filename in sorted(os.listdir(hallway_path)) if os.path.isfile(os.path.join(hallway_path, filename))]\n",
        "\n",
        "hallway_images = []\n",
        "for i in range(0, len(hallway_files)): \n",
        "    img = cv2.imread(os.path.join(hallway_path, hallway_files[i]))\n",
        "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGRA2GRAY)\n",
        "    hallway_images.append(gray_img)\n",
        "\n",
        "hallway_images = np.array(hallway_images)\n",
        "print(hallway_images.shape)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 340, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IkMmSPbWTTX"
      },
      "source": [
        "# i. Read in two images. (Note: if the images are large, you may want to reduce\n",
        "# their size to keep running time reasonable! Document in your report the scale\n",
        "# factor you used.)\n",
        "\n",
        "use_hall = True\n",
        "#use_hall = False\n",
        "if use_hall:\n",
        "    img1, img2 = np.random.default_rng().choice(hallway_images, 2, replace=False)\n",
        "else:\n",
        "    img1, img2 = np.random.default_rng().choice(office_images, 2, replace=False)\n",
        "\n",
        "#print('Original Images:')\n",
        "#cv2_imshow(img1)\n",
        "#print('--------------------------------------------------------------')\n",
        "#cv2_imshow(img2)\n",
        "#print('--------------------------------------------------------------')\n",
        "\n",
        "scale = 0.5\n",
        "r_img1 = cv2.resize(img1, (0,0), fx=scale, fy=scale)\n",
        "r_img2 = cv2.resize(img2, (0,0), fx=scale, fy=scale)\n",
        "\n",
        "print('Reduced Images:')\n",
        "cv2_imshow(r_img1)\n",
        "print('----------------------------------')\n",
        "cv2_imshow(r_img2)\n",
        "print('----------------------------------')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7rOjuT1Dz3K"
      },
      "source": [
        "# ii. Apply Harris corner detector to both images: compute Harris R function \n",
        "# over the image, and then do non-maximum suppression to get a sparse set of \n",
        "# corner features.\n",
        "\n",
        "# https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_features_harris/py_features_harris.html\n",
        "# https://stackoverflow.com/a/56802012\n",
        "\n",
        "# https://stackoverflow.com/questions/6931362/non-maximum-suppression-in-opencv\n",
        "# https://fahmifahim.com/2010/10/22/opencv-corner-detection/\n",
        "\n",
        "\n",
        "def harris1(img):\n",
        "    # Compute the Image Gradient\n",
        "    Ix = cv2.Sobel(img, ddepth=cv2.CV_64F, dx=1, dy=0, ksize=3)\n",
        "    Iy = cv2.Sobel(img, ddepth=cv2.CV_64F, dx=0, dy=1, ksize=3)\n",
        "\n",
        "    # Compute products of derivatives at each pixel\n",
        "    I2x = Ix * Ix\n",
        "    I2y = Iy * Iy\n",
        "    Ixy = Ix * Iy\n",
        "\n",
        "    # Compute the sums of the products at each pixel using a window averaging:\n",
        "    gauss_window = signal.windows.gaussian(3 * 3, std=1, sym=True)\n",
        "    gauss_window = np.reshape(gauss_window, (3, 3))\n",
        "\n",
        "    S2x = signal.convolve2d(gauss_window, I2x) # Gs' * I2x\n",
        "    S2y = signal.convolve2d(gauss_window, I2y) # Gs’ * I2y \n",
        "    Sxy = signal.convolve2d(gauss_window, Ixy) # Gs’ * Ixy\n",
        "\n",
        "    # Define the Matrix at each pixel\n",
        "    M_top = np.hstack((S2x, Sxy))\n",
        "    M_bot = np.hstack((Sxy, S2y))\n",
        "    M = np.vstack((M_top, M_bot)) # [S2x Sxy ; Sxy S2y]\n",
        "    #print(M)\n",
        "\n",
        "    # Compute the response\n",
        "    k = 0.05\n",
        "    det_M = (S2x * S2y) - (Sxy * Sxy)\n",
        "    trace_M = (S2x + S2y)\n",
        "    R = det_M - k * (trace_M)**2\n",
        "    print('R:')\n",
        "    #print(R)\n",
        "    cv2_imshow(R)\n",
        "\n",
        "    # Threshold R\n",
        "    ret, R = cv2.threshold(R, 0.1*np.amax(R), 255, 0)\n",
        "    print('Thresholded R:')\n",
        "    #print(R)\n",
        "    cv2_imshow(R)\n",
        "\n",
        "    # Compute Nonmax suppression\n",
        "    image = np.pad(R, ((1, 1), (1, 1)))\n",
        "    local_max = np.zeros_like(image)\n",
        "\n",
        "    for i in range(1,image.shape[0]-1):\n",
        "        for j in range(1,image.shape[1]-1):\n",
        "            window = image[i-1:i+1, j-1:j+1]\n",
        "            if image[i,j] == np.amax(window):\n",
        "                local_max[i,j] = image[i,j]\n",
        "\n",
        "    # remove padding\n",
        "    corners = local_max[2:-2, 2:-2]\n",
        "\n",
        "    return corners, (corners + img)\n",
        "\n",
        "\n",
        "def harris2(img):\n",
        "    # find Harris corners\n",
        "    src = np.float32(img)\n",
        "    dst = cv2.cornerHarris(src, blockSize=2, ksize=3, k=0.05)\n",
        "\n",
        "    # result is dilated for marking the corners\n",
        "    dst = cv2.dilate(dst, None)\n",
        "\n",
        "    # Threshold for an optimal value\n",
        "    ret, dst = cv2.threshold(dst, 0.01*dst.max(), 255, 0)\n",
        "    dst = np.uint8(dst)\n",
        "\n",
        "    # find centroids\n",
        "    ret, labels, stats, centroids = cv2.connectedComponentsWithStats(dst)\n",
        "\n",
        "    # define the criteria to stop and refine the corners\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.001)\n",
        "    corners = cv2.cornerSubPix(img, np.float32(centroids), (5,5), (-1,-1), criteria)\n",
        "\n",
        "    # Now draw them\n",
        "    res = np.hstack((centroids, corners))\n",
        "    res = np.int0(res)\n",
        "    img[res[:,1],res[:,0]] = 255\n",
        "\n",
        "    corners = np.zeros_like(img)\n",
        "    corners[res[:,1],res[:,0]] = 255\n",
        "    return corners, img\n",
        "\n",
        "# -----------------------------------------\n",
        "\n",
        "corners1, h_img1 = harris1(r_img1)\n",
        "corners2, h_img2 = harris1(r_img2)\n",
        "#cv2_imshow(corners1)\n",
        "#cv2_imshow(corners2)\n",
        "print('----------------------------------')\n",
        "\n",
        "#corners1, h_img1 = harris2(r_img1)\n",
        "#corners2, h_img2 = harris2(r_img2)\n",
        "\n",
        "print('Harris Corners:')\n",
        "cv2_imshow(corners1)\n",
        "cv2_imshow(h_img1)\n",
        "print('----------------------------------')\n",
        "cv2_imshow(corners2)\n",
        "cv2_imshow(h_img2)\n",
        "print('----------------------------------')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLlNSqeKD9OC"
      },
      "source": [
        "# iii. Find correspondences between the two images: given two set of corners \n",
        "# from the two images, compute normalized cross correlation (NCC) of image \n",
        "# patches centered at each corner. (Note that this will be O(n2) process.) \n",
        "# Choose potential corner matches by finding pair of corners (one from each \n",
        "# image) such that they have the highest NCC value. You may also set a \n",
        "# threshold to keep only matches that have a large NCC score.\n",
        "\n",
        "# https://stackoverflow.com/questions/53463087/python-opencv-search-correspondences-of-2-images-with-harris-corner-detection\n",
        "\n",
        "def corner_coords(corners):\n",
        "    # get corner locations\n",
        "    idx = np.where(corners > 0)\n",
        "    idx = np.array(idx).T\n",
        "    return idx\n",
        "\n",
        "def get_patch(img, h, w, n):\n",
        "    # get n x n patch of image centered at the corner coordinate\n",
        "    pad_img = np.pad(img, ((n, n), (n, n)))\n",
        "    patch = pad_img[h+n : h+2*n, w+n : w+2*n]\n",
        "    return patch\n",
        "\n",
        "def match_patches(img1, img2, corners1, corners2):\n",
        "    # match corners between two image\n",
        "    matches = np.zeros((np.sum(corners1>0), 5))\n",
        "\n",
        "    coords1 = corner_coords(corners1)\n",
        "    coords2 = corner_coords(corners2)\n",
        "\n",
        "    for i in range(0, len(coords1)):\n",
        "        h1, w1 = coords1[i]\n",
        "        match = np.array([h1,w1, 0,0, 0])\n",
        "        patch1 = get_patch(img1, h1, w1, 5)\n",
        "\n",
        "        for j in range(0, len(coords2)):\n",
        "            h2, w2 = coords2[j]\n",
        "            patch2 = get_patch(img2, h2, w2, 5)\n",
        "\n",
        "            #stacked_imgs = np.hstack([patch1, patch2])\n",
        "            #cv2_imshow(stacked_imgs)\n",
        "\n",
        "            # NCC\n",
        "            '''\n",
        "            len_1 = np.sqrt(np.sum(patch1**2))\n",
        "            norm_1 = patch1/len_1\n",
        "\n",
        "            len_2 = np.sqrt(np.sum(patch2**2))\n",
        "            norm_2 = patch2/len_2\n",
        "\n",
        "            norm_1 = np.reshape(norm_1, [1, patch1.size])\n",
        "            norm_2 = np.reshape(norm_2, [patch2.size, 1])\n",
        "\n",
        "            ncc = np.float(np.dot(norm_1, norm_2))\n",
        "            '''\n",
        "            ncc = cv2.matchTemplate(patch1.astype(np.float32), \n",
        "                                    patch2.astype(np.float32), \n",
        "                                    cv2.TM_CCORR_NORMED)\n",
        "            #'''\n",
        "\n",
        "            if ncc >= match[4]:\n",
        "                #print(i, j)\n",
        "                #cv2_imshow(stacked_imgs)\n",
        "                match = np.array([h1,w1, h2,w2, ncc])\n",
        "\n",
        "        matches[i,:] = match\n",
        "\n",
        "    return matches\n",
        "\n",
        "def plot_matches(img1, img2, matches):\n",
        "    # draw lines between matching patches\n",
        "    h_offset = img1.shape[0]\n",
        "    w_offset = img1.shape[1]\n",
        "\n",
        "    print(np.round(matches,3))\n",
        "    x = np.copy(matches[:,0:4:2])\n",
        "    #x[:,1] = x[:,1] + h_offset\n",
        "    #print(x)\n",
        "    y = np.copy(matches[:,1:4:2])\n",
        "    y[:,1] = y[:,1] + w_offset\n",
        "    #print(y)\n",
        "\n",
        "    #stacked_imgs = np.vstack([img1, img2])\n",
        "    stacked_imgs = np.hstack([img1, img2])\n",
        "    plt.imshow(stacked_imgs, cmap='gray')\n",
        "\n",
        "    for i in range(0, x.shape[0]):\n",
        "        if matches[i,-1] >= 0.99:\n",
        "            # only draw really good matches\n",
        "            plt.plot(y[i,:], x[i,:])\n",
        "    \n",
        "    return\n",
        "\n",
        "matches = match_patches(h_img1, h_img2, corners1, corners2)\n",
        "plot_matches(h_img1, h_img2, matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De7Pd1aVF4XV"
      },
      "source": [
        "# iv. Estimate the homography using the above correspondences. Note that these \n",
        "# correspondences are likely to have many errors (outliers). That is ok: you \n",
        "# should use RANSAC to robustly estimate the homography from the noisy \n",
        "# correspondences:\n",
        "\n",
        "# A. Repeatedly sample minimal number of points needed to estimate a homography\n",
        "# (4 pts in this case).\n",
        "\n",
        "# B. Compute a homography from these four points.\n",
        "\n",
        "# C. Map all points using the homography and comparing distances between \n",
        "# predicted and observed locations to determine the number of inliers.\n",
        "\n",
        "# D. At the end, compute a least-squares homgraphy from ALL the inliers in the\n",
        "# largest set of inliers.\n",
        "\n",
        "# https://stackoverflow.com/questions/38491959/how-to-apply-ransac-in-python-opencv\n",
        "# https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_feature_homography/py_feature_homography.html\n",
        "\n",
        "def RANSAC(img1, img2):\n",
        "    # Initiate SIFT detector\n",
        "    sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "    # find the keypoints and descriptors with SIFT\n",
        "    img1 = cv2.normalize(img1, None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
        "    img2 = cv2.normalize(img2, None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
        "    mask1 = (corners1[:, :, np.newaxis]).astype('uint8')\n",
        "    mask1[np.where(mask1 > 0)] = 1\n",
        "    mask2 = (corners2[:, :, np.newaxis]).astype('uint8')\n",
        "    mask2[np.where(mask2 > 0)] = 1\n",
        "    #print(mask1.shape)\n",
        "    #print(mask2.shape)\n",
        "\n",
        "    kp1, des1 = sift.detectAndCompute(img1, mask=mask1)\n",
        "    kp2, des2 = sift.detectAndCompute(img2, mask=mask2)\n",
        "\n",
        "    FLANN_INDEX_KDTREE = 0\n",
        "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
        "    search_params = dict(checks = 50)\n",
        "\n",
        "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "\n",
        "    matches2 = flann.knnMatch(des1, des2, k=2)\n",
        "\n",
        "    # store all the good matches as per Lowe's ratio test.\n",
        "    good = []\n",
        "    for m,n in matches2:\n",
        "        if m.distance < 0.7*n.distance:\n",
        "            good.append(m)\n",
        "\n",
        "    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
        "    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
        "    #print(src_pts)\n",
        "    #print(dst_pts)\n",
        "\n",
        "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
        "    matchesMask = mask.ravel().tolist()\n",
        "\n",
        "    h,w = img1.shape\n",
        "    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
        "    dst = cv2.perspectiveTransform(pts, M)\n",
        "    img2 = cv2.polylines(img2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
        "    \n",
        "    img3 = cv2.drawMatches(img1,kp1,img2,kp2,good,None,matchesMask = matchesMask, flags = 2)\n",
        "\n",
        "    plt.imshow(img3, 'gray'),plt.show()\n",
        "\n",
        "    return M, img3\n",
        "\n",
        "\n",
        "M, ret = RANSAC(r_img1, r_img2)\n",
        "\n",
        "print('----------------------------------')\n",
        "print('Homography Matrix:')\n",
        "print(M)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUrtIPz8si3B"
      },
      "source": [
        "# v. Warp one image onto the other one, blending overlapping pixels together to\n",
        "# create a single image that shows the union of all pixels from both input \n",
        "# images. You can choose which of the images to warp. The steps are as follows:\n",
        "\n",
        "# A. Determine how big to make the final output image so that it contains the\n",
        "# union of all pixels in the two images.\n",
        "\n",
        "# B. Copy the image that does not have to be warped into the appropriate \n",
        "# location in the output.\n",
        "\n",
        "# C. Warp the other image into the output image based on the estimated\n",
        "# homography (or its inverse). You can use matlab functions if you want or \n",
        "# write your own warping function.\n",
        "\n",
        "# D. Use any of the blending schemes we will discuss in class to blend pixels\n",
        "# in the area of overlap between both images.\n",
        "\n",
        "# https://stackoverflow.com/questions/38491959/how-to-apply-ransac-in-python-opencv\n",
        "# https://medium.com/analytics-vidhya/panorama-formation-using-image-stitching-using-opencv-1068a0e8e47b\n",
        "\n",
        "def warp(img1, img2, H):\n",
        "    final_h = img1.shape[0]\n",
        "    final_w = img1.shape[1] + img2.shape[1]\n",
        "\n",
        "    warped_img = cv2.warpPerspective(img1, H, (final_w, final_h))\n",
        "    warped_img[0:img2.shape[0], 0:img2.shape[1]] = img2\n",
        "\n",
        "    return (final_w, final_h), warped_img\n",
        "\n",
        "\n",
        "(final_x, final_y), warped_img = warp(r_img1, r_img2, M)\n",
        "print('Final output image size: (width, height)')\n",
        "print((final_x, final_y))\n",
        "print('----------------------------------')\n",
        "\n",
        "print('Stitched Image:')\n",
        "cv2_imshow(warped_img)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}